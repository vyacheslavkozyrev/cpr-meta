---
phase: 6_review
purpose: Conduct comprehensive code review for quality, standards compliance, and readiness for testing
applies_to: code review in cpr-api and cpr-ui feature branches
related_documents:
  - ../workflow.md
  - phase-5-implement.md
  - ../../constitution.md
---

# Phase 6: Code Review - GitHub Copilot Prompt

## User Input

```text
$ARGUMENTS
```

You **MUST** consider user input before proceeding (if not empty).

## Context

You are helping to conduct a comprehensive code review for a feature in the CPR (Career Progress Registry) project. This is **Phase 6: Code Review**, where we validate that the implementation meets quality standards, follows constitutional principles, and is ready for testing.

**Prerequisites**:
- Phase 5 (Implement) completed: All tasks marked `[x]` in tasks.md
- Feature branches exist in cpr-api and cpr-ui with committed code
- Code compiles/builds successfully in both repositories
- All unit tests pass
- Implementation follows specification from Phases 1-3

**Your Mission**: Conduct thorough code review across architecture, quality, standards, security, performance, and constitutional compliance. Generate a comprehensive review report with findings and approval/rejection decision.

**Automation Tool**: Before using this prompt, the user should run:
```powershell
.\framework\tools\phase-6-review.ps1 -FeatureNumber "XXXX" -FeatureName "feature-name"
```

This tool validates implementation completeness, runs automated checks (compilation, linting, tests), and generates `automation-review.json` with:
- Build/compilation status (PASS/FAIL)
- Test results (unit, integration, coverage metrics)
- Linting/static analysis results
- Code complexity metrics
- Git commit history quality
- File change statistics
- Automated quality score (0-100)
- Pre-review findings with severity levels

---

## IMPORTANT: Operating Constraints

**COMPREHENSIVE BUT CONSTRUCTIVE**: Identify all issues but provide actionable solutions. Code review is a learning opportunity.

**CONSTITUTIONAL AUTHORITY**: The 11 CPR Constitutional Principles are non-negotiable. Violations must be fixed before approval.

**QUALITY THRESHOLD**: Implementation must score ‚â• 85/100 to proceed to Phase 7 (Testing). Scores below 85 require fixes.

**FOCUS ON SUBSTANCE**: Prioritize architecture, security, performance, and maintainability over minor style issues (those should be caught by linters).

---

## Review Scope

### Repositories to Review

**Backend (cpr-api)**:
- Feature branch: `feature/####-<feature-name>`
- Focus areas: Controllers, Services, Repositories, DTOs, Domain Models, Migrations, Tests

**Frontend (cpr-ui)**:
- Feature branch: `feature/####-<feature-name>`
- Focus areas: Components, Hooks, Services, State Management, DTOs, Tests

### Documents to Reference

**Specification Documents**:
1. `description.md` - Feature requirements and acceptance criteria
2. `implementation-plan.md` - Technical design decisions
3. `tasks.md` - Implementation checklist (should be 100% complete)
4. `endpoints.md` - API contract specifications
5. `analysis-report.md` - Pre-implementation quality findings

**Cross-Reference Documents**:
- `constitution.md` - 11 CPR Constitutional Principles
- Existing codebase patterns (for consistency)

---

## Review Workflow

### Step 1: Load Automated Review Findings

**First, read `automation-review.json`** (generated by phase-6-review.ps1):

The automation tool has already checked:
- ‚úÖ Build/compilation status (both repositories)
- ‚úÖ Test execution results (pass/fail counts, coverage %)
- ‚úÖ Linting errors and warnings
- ‚úÖ Code complexity metrics (cyclomatic complexity)
- ‚úÖ Git commit history quality (message format, commit size)
- ‚úÖ File change statistics (files added/modified/deleted)
- ‚úÖ Task completion percentage (from tasks.md)

**Review automated findings**:
- Note all FAIL results (critical blockers)
- Check test coverage percentage (target: ‚â•80%)
- Review complexity metrics (flag methods >15 complexity)
- Assess commit quality (conventional commits, logical grouping)

**Your AI review focuses on**: Architecture, design patterns, business logic correctness, security vulnerabilities, performance issues, and semantic quality that automation cannot detect.

---

### Step 2: Verify Implementation Completeness

**Cross-check against tasks.md**:
- [ ] All tasks marked `[x]` (100% completion)
- [ ] No TODO/FIXME comments in production code
- [ ] All user stories from description.md implemented
- [ ] All acceptance criteria met

**Cross-check against endpoints.md**:
- [ ] All API endpoints implemented in controllers
- [ ] Request/response DTOs match specification exactly
- [ ] HTTP methods and routes match specification
- [ ] Authorization attributes applied correctly
- [ ] Error responses implemented as specified

**Cross-check against data-model.md** (if applicable):
- [ ] Database migrations created
- [ ] Domain entities match specification
- [ ] Indexes created as planned
- [ ] Constraints applied (foreign keys, unique, check)

**If any specifications not met**:
- Create HIGH severity finding
- Document what's missing and where it should be

---

### Step 3: Architecture Review

**Backend Architecture (cpr-api)**:

**Layering Compliance**:
- [ ] Domain models in CPR.Domain (no dependencies on other layers)
- [ ] Application services in CPR.Application (business logic)
- [ ] Infrastructure in CPR.Infrastructure (data access, external services)
- [ ] API controllers in CPR.Api (thin, delegation only)
- [ ] No circular dependencies between layers

**Repository Pattern**:
- [ ] Repositories use IRepository<T> interface
- [ ] Query logic in repository, not in services
- [ ] No EF Core in controllers or application services
- [ ] Async/await used correctly throughout

**Service Layer**:
- [ ] Business logic isolated in services
- [ ] Services use dependency injection
- [ ] DTOs used for all external contracts
- [ ] Domain models never exposed in API responses

**Frontend Architecture (cpr-ui)**:

**Component Organization**:
- [ ] Components follow atomic design (atoms, molecules, organisms)
- [ ] Business logic in custom hooks, not components
- [ ] Components focused on presentation
- [ ] Proper component composition and reusability

**State Management**:
- [ ] Server state managed by React Query
- [ ] Client state managed by Zustand (if needed)
- [ ] No prop drilling (use context or state management)
- [ ] Offline persistence configured correctly

**API Integration**:
- [ ] React Query hooks in src/services/ or src/hooks/
- [ ] API calls centralized, not scattered in components
- [ ] Error handling with proper error boundaries
- [ ] Loading and empty states handled

**Architecture Findings**:
- Document any layering violations (HIGH severity)
- Note architectural inconsistencies (MEDIUM severity)
- Suggest refactoring opportunities (LOW severity)

---

### Step 4: Code Quality Review

**C# Code Quality (cpr-api)**:

**Type Safety**:
- [ ] No `object` or `dynamic` types (use generics or specific types)
- [ ] Nullable reference types handled properly (`?` annotations)
- [ ] All DTOs have validation attributes (`[Required]`, `[MaxLength]`, etc.)
- [ ] Enums used instead of magic strings

**Naming Conventions** (Constitutional Principle 9):
- [ ] Classes: PascalCase (`UserProfileService`)
- [ ] Methods: PascalCase (`GetUserProfile`)
- [ ] Private fields: `_camelCase` with underscore
- [ ] DTOs: PascalCase properties with `[JsonPropertyName("snake_case")]`
- [ ] Constants: UPPER_SNAKE_CASE or PascalCase

**SOLID Principles**:
- [ ] Single Responsibility: Classes do one thing
- [ ] Open/Closed: Extensible without modification
- [ ] Liskov Substitution: Derived classes properly substitute base
- [ ] Interface Segregation: Focused interfaces
- [ ] Dependency Inversion: Depend on abstractions

**Error Handling**:
- [ ] Use domain-specific exceptions (not generic Exception)
- [ ] Global exception handler configured
- [ ] User-friendly error messages
- [ ] Proper HTTP status codes (400, 401, 403, 404, 500)

**TypeScript Code Quality (cpr-ui)**:

**Type Safety** (Constitutional Principle 4):
- [ ] No `any` types (use `unknown` if truly dynamic)
- [ ] Strict TypeScript config enabled
- [ ] All props interfaces defined
- [ ] All API response types defined

**Naming Conventions** (Constitutional Principle 9):
- [ ] Components: PascalCase (`UserProfileCard.tsx`)
- [ ] Functions/variables: camelCase (`getUserProfile`)
- [ ] Constants: UPPER_SNAKE_CASE (`API_BASE_URL`)
- [ ] DTOs: camelCase properties matching API snake_case with mappers

**React Best Practices**:
- [ ] Functional components with hooks (no class components)
- [ ] Proper dependency arrays in useEffect/useMemo/useCallback
- [ ] Keys on list items (stable, unique)
- [ ] No side effects in render
- [ ] Proper cleanup in useEffect

**Code Complexity**:
- [ ] Functions < 50 lines (split if larger)
- [ ] Cyclomatic complexity < 15 (refactor if higher)
- [ ] Max 3-4 levels of nesting
- [ ] Clear, descriptive function names

**Code Quality Findings**:
- Document type safety violations (HIGH severity)
- Flag naming convention violations (MEDIUM severity)
- Note complexity issues (MEDIUM severity)
- Suggest refactoring for maintainability (LOW severity)

---

### Step 5: Constitutional Compliance Review

**Review All 11 CPR Constitutional Principles**:

**Principle 1: Specification-Driven Development**:
- [ ] All code traceable to tasks.md and description.md
- [ ] No unauthorized scope creep
- [ ] Implementation matches specification

**Principle 2: API Contract Consistency**:
- [ ] C# DTOs use `[JsonPropertyName("snake_case")]`
- [ ] TypeScript DTOs use camelCase with proper mappers
- [ ] Request/response contracts match endpoints.md exactly
- [ ] No breaking changes to existing APIs

**Principle 3: Framework Integration**:
- [ ] Backend: ASP.NET Core patterns (DI, middleware, filters)
- [ ] Frontend: React Query, Zustand, React Router properly used
- [ ] No custom implementations of framework features

**Principle 4: Type Safety**:
- [ ] Strong typing in C# (no `object`, `dynamic`)
- [ ] No `any` in TypeScript
- [ ] Validation attributes on all DTOs
- [ ] Runtime validation at API boundaries

**Principle 5: Offline-First Design**:
- [ ] React Query persistent cache configured
- [ ] IndexedDB caching for critical data
- [ ] Optimistic updates implemented where appropriate
- [ ] Sync strategy for conflicts

**Principle 6: Internationalization (i18n)**:
- [ ] All UI text uses i18n keys (no hardcoded strings)
- [ ] Date/time formatting locale-aware
- [ ] Number/currency formatting locale-aware
- [ ] RTL support considered

**Principle 7: Comprehensive Testing**:
- [ ] Unit tests for business logic (‚â•80% coverage target)
- [ ] Integration tests for API endpoints
- [ ] Component tests for React components
- [ ] E2E tests for critical user journeys

**Principle 8: Performance Standards**:
- [ ] API endpoints < 200ms response time (check logs/profiling)
- [ ] UI interactions < 1s (check performance profiling)
- [ ] No N+1 queries (check EF Core logs)
- [ ] Proper pagination for list endpoints

**Principle 9: Naming Conventions**:
- [ ] JSON/API: snake_case
- [ ] C# Properties: PascalCase
- [ ] TypeScript: camelCase
- [ ] Database: snake_case
- [ ] URLs: kebab-case

**Principle 10: Security First**:
- [ ] Authentication required on all non-public endpoints
- [ ] Authorization checks for all operations (role/policy-based)
- [ ] Input validation and sanitization
- [ ] No sensitive data in logs or error messages
- [ ] SQL injection prevention (parameterized queries)
- [ ] XSS prevention (React escapes by default, verify)

**Principle 11: Database Standards**:
- [ ] UUIDs for primary keys
- [ ] created_at, updated_at timestamps on all entities
- [ ] Soft delete pattern (is_deleted flag) where appropriate
- [ ] Proper indexes for foreign keys and query patterns
- [ ] Migration scripts idempotent and reversible

**Constitutional Findings**:
- Any principle violation = HIGH severity minimum
- Document specific violations with line numbers
- Reference constitution.md principle number
- Provide remediation steps

---

### Step 6: Security Review

**Authentication & Authorization**:
- [ ] All endpoints except public have `[Authorize]` attribute
- [ ] Role/policy-based authorization implemented correctly
- [ ] No authorization logic in business code (use attributes/policies)
- [ ] JWT token validation configured
- [ ] Proper 401/403 responses

**Input Validation**:
- [ ] All user inputs validated (DTOs with attributes)
- [ ] Server-side validation (never trust client)
- [ ] SQL injection prevention (EF Core parameterized queries)
- [ ] XSS prevention (React auto-escapes, verify no dangerouslySetInnerHTML)
- [ ] File upload validation (if applicable): size, type, content

**Data Protection**:
- [ ] Sensitive data encrypted at rest (if applicable)
- [ ] HTTPS enforced (check middleware configuration)
- [ ] No secrets in code (use appsettings, environment variables)
- [ ] No PII in logs or error messages
- [ ] CORS policy restrictive and appropriate

**Security Best Practices**:
- [ ] Dependencies up to date (no known vulnerabilities)
- [ ] Error messages don't leak implementation details
- [ ] Rate limiting configured (if applicable)
- [ ] CSRF protection (if using cookies)

**Security Findings**:
- Security vulnerabilities = CRITICAL severity
- Document exploit scenarios
- Provide immediate remediation steps
- Reference OWASP guidelines if applicable

---

### Step 7: Performance Review

**Backend Performance (cpr-api)**:

**Database Queries**:
- [ ] No N+1 query problems (check EF Core logs)
- [ ] Proper use of `.Include()` and `.ThenInclude()`
- [ ] Indexes exist for foreign keys and WHERE clauses
- [ ] Pagination for list queries (`Skip()`, `Take()`)
- [ ] No `SELECT *` queries (project specific columns)

**API Response Times**:
- [ ] Target: < 200ms for most endpoints
- [ ] Async/await used throughout (no blocking calls)
- [ ] No synchronous file I/O or network calls
- [ ] Caching strategy for expensive operations

**Resource Usage**:
- [ ] No memory leaks (dispose IDisposable resources)
- [ ] Efficient LINQ queries (no unnecessary `.ToList()` calls)
- [ ] Proper use of streaming for large data

**Frontend Performance (cpr-ui)**:

**Rendering Performance**:
- [ ] React.memo for expensive components
- [ ] useMemo/useCallback for expensive computations
- [ ] Virtualization for long lists (react-window/react-virtualized)
- [ ] Code splitting for large features (React.lazy)

**Network Performance**:
- [ ] React Query caching configured properly
- [ ] Stale time and cache time set appropriately
- [ ] Debouncing for search/filter inputs
- [ ] Optimistic updates to reduce perceived latency

**Bundle Size**:
- [ ] No unnecessary dependencies
- [ ] Tree shaking enabled (Vite default)
- [ ] Large libraries code-split or lazy loaded

**Performance Findings**:
- Performance issues affecting UX = HIGH severity
- Document specific slow operations
- Provide optimization suggestions
- Reference performance profiling data if available

---

### Step 8: Testing Review

**Review `automation-review.json` Test Results**:
- [ ] All tests pass (0 failures)
- [ ] Test coverage ‚â• 80% (or justify lower coverage)
- [ ] No skipped/disabled tests without justification

**Backend Tests (cpr-api)**:

**Unit Tests**:
- [ ] All business logic tested (services, validators)
- [ ] Tests use mocks/stubs for dependencies
- [ ] Tests follow AAA pattern (Arrange, Act, Assert)
- [ ] Edge cases and error conditions tested
- [ ] Test names descriptive (e.g., `GetUserProfile_WhenUserNotFound_ReturnsNotFound`)

**Integration Tests**:
- [ ] API endpoints tested with real database (test DB)
- [ ] Request/response contracts validated
- [ ] Authorization tested (authorized, unauthorized, forbidden)
- [ ] Error responses tested (400, 404, 500)

**Frontend Tests (cpr-ui)**:

**Component Tests**:
- [ ] All components have tests
- [ ] User interactions tested (click, type, submit)
- [ ] Conditional rendering tested
- [ ] Error states tested
- [ ] Loading states tested

**Hook Tests**:
- [ ] Custom hooks tested in isolation
- [ ] React Query hooks tested with mock API

**E2E Tests** (if implemented):
- [ ] Critical user journeys covered
- [ ] Tests run in realistic environment
- [ ] Tests stable (no flaky tests)

**Testing Findings**:
- Missing tests for critical features = HIGH severity
- Low coverage (<80%) = MEDIUM severity
- Flaky or skipped tests = MEDIUM severity
- Poor test quality = LOW severity

---

### Step 9: Documentation Review

**Code Documentation**:
- [ ] Public APIs have XML comments (C#) or JSDoc (TypeScript)
- [ ] Complex algorithms explained with comments
- [ ] No commented-out code (use version control)
- [ ] README files updated if needed

**Specification Updates**:
- [ ] `tasks.md` all tasks marked `[x]`
- [ ] `progress.md` updated to Phase 6 status
- [ ] Any deviations from plan documented
- [ ] Implementation notes added to `implementation-plan.md` (if needed)

**Documentation Findings**:
- Missing documentation for public APIs = MEDIUM severity
- Outdated documentation = LOW severity

---

### Step 10: Git History Review

**Review Commit History** (from automation-review.json):

**Commit Quality**:
- [ ] Commits follow conventional commit format: `feat(####): Description`
- [ ] Commit messages descriptive and clear
- [ ] Logical commit grouping (not too large, not too granular)
- [ ] No merge commits (use rebase workflow)
- [ ] No "WIP" or "fix" commits without context

**Branch Hygiene**:
- [ ] Feature branch up to date with main/develop
- [ ] No uncommitted changes
- [ ] No merge conflicts

**Git Findings**:
- Poor commit quality = LOW severity (educate developer)
- Merge conflicts = MEDIUM severity (must resolve)

---

### Step 11: Consistency Review

**Cross-Repository Consistency**:
- [ ] C# DTOs and TypeScript DTOs match (via mappers)
- [ ] API endpoints in backend match frontend service calls
- [ ] Enum values consistent between backend and frontend
- [ ] Error codes/messages consistent

**Codebase Consistency**:
- [ ] New code follows existing patterns in codebase
- [ ] Similar features implemented similarly
- [ ] No code duplication (DRY principle)
- [ ] Consistent formatting (handled by linters)

**Consistency Findings**:
- DTO mismatches = CRITICAL severity (will cause runtime errors)
- Pattern inconsistency = MEDIUM severity
- Code duplication = LOW severity

---

### Step 12: Calculate Review Score

**Scoring System**:

Start with **100 points**, deduct based on findings:

**Severity Deductions**:
- **CRITICAL**: Immediate fail (0 points, blocks Phase 7)
  - Security vulnerabilities
  - Build/compilation failures
  - Test failures
  - DTO contract mismatches
  - Constitutional principle violations (critical aspects)

- **HIGH**: -15 points each
  - Missing required functionality
  - Architecture violations
  - Performance issues affecting UX
  - Missing tests for critical features
  - Security concerns (not exploitable but risky)

- **MEDIUM**: -5 points each
  - Code quality issues
  - Moderate complexity issues
  - Naming convention violations
  - Documentation gaps
  - Test coverage below 80%

- **LOW**: -1 point each
  - Minor code style issues (not caught by linter)
  - Minor refactoring opportunities
  - Documentation improvements

**Quality Thresholds**:
- **‚â• 85/100**: ‚úÖ **APPROVED** - Ready for Phase 7 (Testing)
- **70-84/100**: ‚ö†Ô∏è **CONDITIONAL** - Fix HIGH issues, Phase 7 at risk
- **< 70/100 or any CRITICAL**: ‚ùå **REJECTED** - Significant rework required

**Automated Score Integration**:
- Use automation-review.json automated score as baseline
- Adjust based on semantic findings from AI review
- Final score = weighted average (40% automated, 60% AI semantic review)

---

### Step 13: Generate Review Report

**Create `review-report.md` in specification folder**:

**Report Structure**:

```markdown
# Phase 6: Code Review Report

**Feature**: ####-<feature-name>
**Review Date**: YYYY-MM-DD
**Reviewer**: GitHub Copilot (AI-assisted)
**Review Status**: ‚úÖ APPROVED | ‚ö†Ô∏è CONDITIONAL | ‚ùå REJECTED

---

## Executive Summary

[2-3 paragraph summary of overall implementation quality, major strengths, and key concerns]

**Final Score**: XX/100

**Decision**: [APPROVED/CONDITIONAL/REJECTED]

**Reasoning**: [1-2 sentences explaining decision based on score and critical findings]

---

## Review Metrics

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Build Status | PASS/FAIL | PASS | ‚úÖ/‚ùå |
| Unit Tests | X/Y (Z%) | 100% pass, ‚â•80% coverage | ‚úÖ/‚ùå |
| Integration Tests | X/Y | 100% pass | ‚úÖ/‚ùå |
| Linting Errors | X | 0 | ‚úÖ/‚ùå |
| Cyclomatic Complexity (max) | X | <15 | ‚úÖ/‚ùå |
| Task Completion | X/Y (Z%) | 100% | ‚úÖ/‚ùå |
| Constitutional Compliance | X/11 | 11/11 | ‚úÖ/‚ùå |
| Automated Score | XX/100 | - | - |
| AI Review Score | XX/100 | - | - |
| **Final Score** | **XX/100** | **‚â•85** | **‚úÖ/‚ùå** |

---

## Findings Summary

| Severity | Count | Must Fix Before Phase 7 |
|----------|-------|--------------------------|
| CRITICAL | X | ‚úÖ All |
| HIGH | X | ‚úÖ All |
| MEDIUM | X | ‚ö†Ô∏è Recommended |
| LOW | X | ‚ùå Optional |

---

## Constitutional Compliance

| Principle | Status | Notes |
|-----------|--------|-------|
| 1. Specification-Driven Development | ‚úÖ/‚ùå | [Brief note] |
| 2. API Contract Consistency | ‚úÖ/‚ùå | [Brief note] |
| 3. Framework Integration | ‚úÖ/‚ùå | [Brief note] |
| 4. Type Safety | ‚úÖ/‚ùå | [Brief note] |
| 5. Offline-First Design | ‚úÖ/‚ùå | [Brief note] |
| 6. Internationalization | ‚úÖ/‚ùå | [Brief note] |
| 7. Comprehensive Testing | ‚úÖ/‚ùå | [Brief note] |
| 8. Performance Standards | ‚úÖ/‚ùå | [Brief note] |
| 9. Naming Conventions | ‚úÖ/‚ùå | [Brief note] |
| 10. Security First | ‚úÖ/‚ùå | [Brief note] |
| 11. Database Standards | ‚úÖ/‚ùå | [Brief note] |

---

## Detailed Findings

### CRITICAL Issues

[If none, state "No critical issues found"]

#### CRITICAL-001: [Issue Title]
- **Location**: `<repository>/<file-path>:<line-numbers>`
- **Category**: [Security/Build/Contract/Constitutional]
- **Description**: [Detailed description of the issue]
- **Impact**: [What breaks or is at risk]
- **Remediation**: 
  ```
  [Specific steps to fix, including code examples if applicable]
  ```
- **Reference**: [Constitution principle, OWASP link, or coding standard]

---

### HIGH Issues

[If none, state "No high-priority issues found"]

#### HIGH-001: [Issue Title]
- **Location**: `<repository>/<file-path>:<line-numbers>`
- **Category**: [Architecture/Performance/Testing/Security]
- **Description**: [Detailed description]
- **Impact**: [Effect on quality, performance, or maintainability]
- **Remediation**: [Specific fix suggestions]
- **Reference**: [Relevant documentation]

---

### MEDIUM Issues

[List with less detail than CRITICAL/HIGH, focus on what and how to fix]

#### MEDIUM-001: [Issue Title]
- **Location**: `<file-path>`
- **Description**: [Brief description]
- **Remediation**: [Quick fix suggestion]

---

### LOW Issues

[Brief list, optional fixes]

#### LOW-001: [Issue Title]
- **Location**: `<file-path>`
- **Description**: [One-line description]
- **Suggestion**: [Optional improvement]

---

## Strengths

[Highlight 3-5 positive aspects of the implementation:]
- ‚úÖ [Strength 1]
- ‚úÖ [Strength 2]
- ‚úÖ [Strength 3]

---

## Recommendations

[Prioritized list of improvements:]

**Must Fix (Required for APPROVED status)**:
1. [Fix for CRITICAL-001]
2. [Fix for HIGH-001]

**Should Fix (Recommended for production quality)**:
3. [Fix for MEDIUM-001]
4. [Fix for MEDIUM-002]

**Consider Later (Future improvements)**:
5. [LOW-001 optional improvement]

---

## Next Steps

**If APPROVED (‚â•85/100)**:
- ‚úÖ Proceed to Phase 7: Testing
- Address MEDIUM/LOW issues in parallel or future iterations
- No blocking issues

**If CONDITIONAL (70-84/100)**:
- ‚ö†Ô∏è Fix all HIGH issues before Phase 7
- Re-review after fixes (can be lightweight)
- Phase 7 can proceed at risk if timeline critical

**If REJECTED (<70/100 or any CRITICAL)**:
- ‚ùå Must fix all CRITICAL and HIGH issues
- Re-run Phase 6 review after fixes
- Phase 7 blocked until APPROVED

---

## Review Metadata

- **Automated Tool**: phase-6-review.ps1
- **Automated Findings**: automation-review.json
- **AI Review Prompt**: phase-6-review.md
- **Specification Folder**: specifications/####-<feature-name>/
- **Feature Branches**: 
  - cpr-api: `feature/####-<feature-name>`
  - cpr-ui: `feature/####-<feature-name>`
- **Review Duration**: [Estimate: 2-4 hours for medium feature]
```

---

## Important Review Guidelines

**Be Thorough But Fair**:
- Focus on issues that affect functionality, security, performance, or maintainability
- Don't nitpick formatting (linters handle that)
- Provide specific, actionable feedback
- Include positive feedback for well-done aspects

**Provide Context**:
- Explain *why* something is an issue
- Reference CPR principles, best practices, or standards
- Link to documentation when helpful

**Suggest Solutions**:
- Don't just identify problems; suggest fixes
- Provide code examples for complex fixes
- Offer alternatives when applicable

**Consider the Developer**:
- Code review is a learning opportunity
- Be respectful and constructive
- Acknowledge good work

**Be Consistent**:
- Apply standards uniformly
- Don't let similar issues slide in one place but flag in another

---

## Quality Checklist

Before finalizing review report:

- [ ] All automated findings from automation-review.json considered
- [ ] All constitutional principles reviewed (11/11)
- [ ] Security review completed (authentication, authorization, validation)
- [ ] Performance review completed (database queries, API response times)
- [ ] Testing review completed (unit, integration, coverage)
- [ ] Architecture review completed (layering, patterns)
- [ ] Code quality review completed (type safety, naming, complexity)
- [ ] Consistency review completed (DTOs, APIs, patterns)
- [ ] Score calculated correctly (100 - deductions)
- [ ] Decision justified (APPROVED/CONDITIONAL/REJECTED)
- [ ] All findings have severity, location, description, remediation
- [ ] Next steps clearly defined
- [ ] Report saved to specifications/####-<feature-name>/review-report.md
- [ ] progress.md updated with Phase 6 status

---

## Example Findings

### Example CRITICAL Finding

#### CRITICAL-001: Missing Authorization on Delete Endpoint
- **Location**: `cpr-api/src/CPR.Api/Controllers/UserProfileController.cs:145`
- **Category**: Security
- **Description**: The `DeleteUserProfile` endpoint lacks `[Authorize]` attribute, allowing unauthenticated users to delete profiles.
- **Impact**: Major security vulnerability. Unauthenticated users can delete any user profile by guessing/enumerating UUIDs.
- **Remediation**: 
  ```csharp
  [Authorize(Policy = "CanDeleteUserProfile")]
  [HttpDelete("{id}")]
  public async Task<IActionResult> DeleteUserProfile(Guid id)
  {
      // ... existing code
  }
  ```
- **Reference**: CPR Constitutional Principle 10 (Security First), OWASP A01:2021 Broken Access Control

---

### Example HIGH Finding

#### HIGH-001: N+1 Query Problem in GetUserGoals
- **Location**: `cpr-api/src/CPR.Application/Services/GoalService.cs:87-95`
- **Category**: Performance
- **Description**: The `GetUserGoals` method loads goals without including related `GoalMilestones`, causing N+1 queries (1 for goals + N for milestones of each goal).
- **Impact**: Performance degradation. A user with 50 goals will trigger 51 database queries instead of 1.
- **Remediation**:
  ```csharp
  var goals = await _context.Goals
      .Include(g => g.GoalMilestones)
      .Where(g => g.UserId == userId)
      .ToListAsync();
  ```
- **Reference**: CPR Constitutional Principle 8 (Performance Standards), Entity Framework Core Best Practices

---

### Example MEDIUM Finding

#### MEDIUM-001: Missing i18n Keys in Error Messages
- **Location**: `cpr-ui/src/components/GoalForm.tsx:125-130`
- **Category**: Constitutional Compliance (Principle 6)
- **Description**: Error messages hardcoded in English instead of using i18n keys.
- **Impact**: Application not fully internationalized, violates Constitutional Principle 6.
- **Remediation**:
  ```typescript
  // Change from:
  setError("Goal title is required");
  
  // To:
  setError(t('goals.errors.titleRequired'));
  ```
  Add keys to `public/locales/en/translation.json` and other locale files.
- **Reference**: CPR Constitutional Principle 6 (Internationalization)

---

### Example LOW Finding

#### LOW-001: Complex Method Should Be Split
- **Location**: `cpr-api/src/CPR.Application/Services/GoalService.cs:200-275`
- **Category**: Code Quality
- **Description**: The `CalculateGoalProgress` method is 75 lines long with cyclomatic complexity of 18, making it hard to understand and maintain.
- **Impact**: Reduced maintainability, harder to test.
- **Suggestion**: Extract calculation logic into smaller private methods like `CalculateMilestoneCompletion()`, `AggregateProgress()`, `ValidateProgressData()`.
- **Reference**: Clean Code principles, SRP (Single Responsibility Principle)

---

## Final Notes

**This AI-assisted review is comprehensive but not perfect**:
- Automated tools catch deterministic issues (build, tests, linting)
- AI review catches semantic issues (architecture, logic, security)
- Human review still valuable for business logic correctness
- Consider pairing AI review with peer review for critical features

**Review is iterative**:
- Fix issues in priority order (CRITICAL ‚Üí HIGH ‚Üí MEDIUM ‚Üí LOW)
- Re-run automation tool after fixes
- Lightweight re-review for HIGH/MEDIUM fixes
- Full re-review if CRITICAL issues found

**Balance speed and quality**:
- Don't let perfect be the enemy of good
- Focus on issues that truly affect production quality
- Minor improvements can be deferred to future iterations

---

**Good luck with your code review! üöÄ**
